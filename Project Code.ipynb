{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c36492b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from BODSDataExtractor.extractor import TimetableExtractor\n",
    "from datetime import datetime\n",
    "import glob\n",
    "# import mysql.connector\n",
    "# import sqlalchemy \n",
    "import shutil\n",
    "import datetime\n",
    "import os\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74b1f90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in e:\\1231421\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: joblib>=0.11 in e:\\1231421\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in e:\\1231421\\lib\\site-packages (from scikit-learn) (1.21.5)\n",
      "Requirement already satisfied: scipy>=1.1.0 in e:\\1231421\\lib\\site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in e:\\1231421\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\1231421\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\1231421\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\1231421\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\1231421\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\1231421\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\1231421\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e5df028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sqlalchemy in c:\\anaconda3\\lib\\site-packages (1.4.39)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\anaconda3\\lib\\site-packages (from sqlalchemy) (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ce2118e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\诺丁汉大学 学习资料\\诺丁汉大学  计算机科学 学习资料\\毕业设计\\毕业设计——BODS CODE + DATA\n"
     ]
    }
   ],
   "source": [
    "# 更改当前路径\n",
    "# change path\n",
    "new_path = \"D:\\诺丁汉大学 学习资料\\诺丁汉大学  计算机科学 学习资料\\毕业设计\\毕业设计——BODS CODE + DATA\"\n",
    "os.chdir(new_path)\n",
    "current_path = os.getcwd()\n",
    "print(current_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8ac4693",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = 'bbb630ca6698171a44ddc9ebe12ad4a5f0d32bb9'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c39b29",
   "metadata": {},
   "source": [
    "# BODS extractor Dataset level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117bbf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#intiate an object instance called my_bus_data_object with desired parameters\n",
    "from BODSDataExtractor.extractor import TimetableExtractor\n",
    "\n",
    "my_bus_data_object = TimetableExtractor(api_key=api # Your API Key Here\n",
    "                                 ,limit=1 # How many datasets to view\n",
    "                                 ,status = 'published' # Only view published datasets\n",
    "                                 ,service_line_level=False # True if you require Service line data \n",
    "                                 ,stop_level=False # True if you require stop level data\n",
    "                                ,nocs=['BRTB','CT4N','NATX','TBTN','SCEM','NOTB','KBUS','ADER']\n",
    "                                 )\n",
    "\n",
    "#save the extracted dataset level data to dataset_level variable\n",
    "dataset_level = my_bus_data_object.metadata\n",
    "\n",
    "#save this data to a csv file in your downloads directory\n",
    "#note that entries in the 'localities' field may be truncated to comply with excel cell limits\n",
    "my_bus_data_object.save_metadata_to_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012fc391",
   "metadata": {},
   "source": [
    "# BODS extractor：Stop Level Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c78686",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#intiate an object instance called my_bus_data_object with desired parameters\n",
    "\n",
    "\n",
    "my_bus_data_object = TimetableExtractor(api_key=api # Your API Key Here\n",
    "#                                   ,limit=1 # How many datasets to view\n",
    "                                 ,status = 'published' # Only view published datasets\n",
    "                                 ,service_line_level=True # True if you require Service line data \n",
    "                                 ,stop_level=True # True if you require stop level data\n",
    "#                                 ,search = \"\",# keywords to filter specific dataset\n",
    "                                ,nocs=['BRTB', 'CT4N', 'TBTN', 'NOTB', 'KBUS']   #ADER 暂时不提取\n",
    "                                        \n",
    "                                 )\n",
    "\n",
    "#save the extracted dataset level data to filtered_dataset_level variable\n",
    "filtered_dataset_level = my_bus_data_object.metadata\n",
    "\n",
    "#save the extracted dataset level data to lcoal csv file\n",
    "my_bus_data_object.save_metadata_to_csv()\n",
    "\n",
    "#save the extracted service line level data to dataset_level variable\n",
    "filtered_service_line_level = my_bus_data_object.service_line_extract\n",
    "\n",
    "#save the extracted service line level data to lcoal csv file\n",
    "my_bus_data_object.save_service_line_extract_to_csv()\n",
    "\n",
    "#stop_level_extract is a dataframe, which contains a collumn of timetables (inbound/outbound) to be saved to csv as follows (saves in project folder)\n",
    "my_bus_data_object.save_timetables()\n",
    "\n",
    "#visualise a particular service line on an interactive map\n",
    "#my_bus_data_object.visualise_service_line('PC0001838:41')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da65c0e1",
   "metadata": {},
   "source": [
    "# extract service line timetables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf6d1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#intiate an object instance called my_bus_data_object with desired parameters\n",
    "\n",
    "\n",
    "my_bus_data_object = TimetableExtractor(api_key=api # Your API Key Here\n",
    "#                                   ,limit=1 # How many datasets to view\n",
    "                                 ,status = 'published' # Only view published datasets\n",
    "                                 ,service_line_level=True # True if you require Service line data \n",
    "                                 ,stop_level=False # True if you require stop level data\n",
    "#                                 ,search = \"\",# keywords to filter specific dataset\n",
    "                                ,nocs=['BRTB','CT4N','NATX','TBTN','SCEM','NOTB','KBUS','ADER']\n",
    "                                 )\n",
    "\n",
    "#save the extracted service line level data to service_line_level variable\n",
    "service_line_level = my_bus_data_object.service_line_extract\n",
    "\n",
    "#note that in downloading the service line level data, the dataset level will also be downloaded. Can access this as below:\n",
    "dataset_level = my_bus_data_object.metadata\n",
    "\n",
    "#save these data to a csv file in your downloads directory\n",
    "my_bus_data_object.save_metadata_to_csv()\n",
    "my_bus_data_object.save_service_line_extract_to_csv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d01993e",
   "metadata": {},
   "source": [
    "# Transfer XML to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3865618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XML TO CSV\n",
    "\n",
    "import xml.etree.ElementTree as xml\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "\n",
    "target_features = [\"RecordedAtTime\", \"ItemIdentifier\", \"LineRef\", \"DirectionRef\", \"DatedVehicleJourneyRef\", \"PublishedLineName\", \"OperatorRef\", \"OriginRef\", \"DestinationRef\", \"OriginAimedDepartureTime\",\"Longitude\", \"Latitude\",\"DestinationAimedArrivalTime\", \"VehicleRef\", \"JourneyCode\", \"VehicleUniqueId\"]\n",
    "\n",
    "\n",
    "def parse_xml(file):\n",
    "    # 定义命名空间前缀和URI\n",
    "    ns = {\"siri\": \"http://www.siri.org.uk/siri\"}\n",
    "\n",
    "    tree = xml.parse(file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    video = []\n",
    "\n",
    "    for activity in root.findall(\"siri:ServiceDelivery/siri:VehicleMonitoringDelivery/siri:VehicleActivity\", ns):\n",
    "        current_frame = {}\n",
    "        for key in target_features:\n",
    "            # 使用命名空间前缀来匹配标签\n",
    "            element = activity.find(f\"siri:{key}\", ns)\n",
    "            current_frame[key] = element.text if element is not None else \"\"\n",
    "\n",
    "        # 在activity元素下查找第一个MonitoredVehicleJourney元素，并提取对应的变量数据\n",
    "        monitored_journey = activity.find(\"siri:MonitoredVehicleJourney\", ns)\n",
    "        if monitored_journey is not None:\n",
    "            current_frame[\"LineRef\"] = monitored_journey.find(\"siri:LineRef\", ns).text if monitored_journey.find(\"siri:LineRef\", ns) is not None else \"\"\n",
    "            current_frame[\"DirectionRef\"] = monitored_journey.find(\"siri:DirectionRef\", ns).text if monitored_journey.find(\"siri:DirectionRef\", ns) is not None else \"\"\n",
    "            current_frame[\"DatedVehicleJourneyRef\"] = monitored_journey.find(\"siri:FramedVehicleJourneyRef/siri:DatedVehicleJourneyRef\", ns).text if monitored_journey.find(\"siri:FramedVehicleJourneyRef/siri:DatedVehicleJourneyRef\", ns) is not None else \"\"\n",
    "            current_frame[\"PublishedLineName\"] = monitored_journey.find(\"siri:PublishedLineName\", ns).text if monitored_journey.find(\"siri:PublishedLineName\", ns) is not None else \"\"\n",
    "            current_frame[\"OperatorRef\"] = monitored_journey.find(\"siri:OperatorRef\", ns).text if monitored_journey.find(\"siri:OperatorRef\", ns) is not None else \"\"\n",
    "            current_frame[\"OriginRef\"] = monitored_journey.find(\"siri:OriginRef\", ns).text if monitored_journey.find(\"siri:OriginRef\", ns) is not None else \"\"\n",
    "            current_frame[\"DestinationRef\"] = monitored_journey.find(\"siri:DestinationRef\", ns).text if monitored_journey.find(\"siri:DestinationRef\", ns) is not None else \"\"\n",
    "            current_frame[\"OriginAimedDepartureTime\"] = monitored_journey.find(\"siri:OriginAimedDepartureTime\", ns).text if monitored_journey.find(\"siri:OriginAimedDepartureTime\", ns) is not None else \"\"\n",
    "            current_frame[\"Longitude\"] = monitored_journey.find(\"siri:VehicleLocation/siri:Longitude\", ns).text if monitored_journey.find(\"siri:VehicleLocation/siri:Longitude\", ns) is not None else \"\"\n",
    "            current_frame[\"Latitude\"] = monitored_journey.find(\"siri:VehicleLocation/siri:Latitude\", ns).text if monitored_journey.find(\"siri:VehicleLocation/siri:Latitude\", ns) is not None else \"\"\n",
    "            \n",
    "            current_frame[\"DestinationAimedArrivalTime\"] = monitored_journey.find(\"siri:DestinationAimedArrivalTime\", ns).text if monitored_journey.find(\"siri:DestinationAimedArrivalTime\", ns) is not None else \"\"\n",
    "            current_frame[\"VehicleRef\"] = monitored_journey.find(\"siri:VehicleRef\", ns).text if monitored_journey.find(\"siri:VehicleRef\", ns) is not None else \"\"\n",
    "\n",
    "            # 在Extensions下查找VehicleJourney元素\n",
    "            extensions = activity.find(\"siri:Extensions\", ns)\n",
    "            if extensions is not None:\n",
    "                vehicle_journey = extensions.find(\"siri:VehicleJourney\", ns)\n",
    "                if vehicle_journey is not None:\n",
    "                    # 使用命名空间前缀来匹配JourneyCode和VehicleUniqueId标签\n",
    "                    current_frame[\"JourneyCode\"] = vehicle_journey.find(\"siri:Operational/siri:TicketMachine/siri:JourneyCode\", ns).text if vehicle_journey.find(\"siri:Operational/siri:TicketMachine/siri:JourneyCode\", ns) is not None else \"\"\n",
    "                    current_frame[\"VehicleUniqueId\"] = vehicle_journey.find(\"siri:VehicleUniqueId\", ns).text if vehicle_journey.find(\"siri:VehicleUniqueId\", ns) is not None else \"\"\n",
    "\n",
    "        video.append(current_frame)\n",
    "\n",
    "    return video\n",
    "\n",
    "\n",
    "def savetoCSV(video, output):\n",
    "    if video:\n",
    "        with open(output, 'w', newline='') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, target_features)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(video)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56d0762",
   "metadata": {},
   "source": [
    "# Extract SIRI Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "239ed2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to csv...\n",
      "saved to csv...\n",
      "saved to csv...\n",
      "saved to csv...\n",
      "saved to csv...\n",
      "saved to csv...\n",
      "saved to csv...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13424\\3779622534.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'saved to csv...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m120\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Real-time Bus Data\n",
    "import xml.etree.ElementTree as ET\n",
    "import zipfile\n",
    "from bods_client.client import BODSClient\n",
    "from bods_client.models import BoundingBox, SIRIVMParams\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "class Siri:\n",
    "\n",
    "    def __init__(self, xml):\n",
    "        self.xml = xml\n",
    "        self.dict = self.parse(self.xml)\n",
    "\n",
    "    def parse(self, raw_xml):\n",
    "        tree = ET.ElementTree(ET.fromstring(raw_xml))\n",
    "        root = tree.getroot()\n",
    "        return self.parse_element(root)\n",
    "\n",
    "    #处理XML元素结构，返回一个字典变量，key为XML元素名，value为XML元素值\n",
    "    def parse_element(self, element):\n",
    "        #data存储解析后的XML数据\n",
    "        data = {}                         \n",
    "        \n",
    "        #遍历XML元素的所有子元素\n",
    "        for current in element:\n",
    "            #获取子元素的标签名\n",
    "            name = current.tag[29:]\n",
    "\n",
    "            if current:\n",
    "                if name in data:\n",
    "                    if not isinstance(data[name], list):\n",
    "                        temp = data[name]\n",
    "                        data[name] = [temp]\n",
    "                    data[name].append(self.parse_element(current))\n",
    "                else:\n",
    "                    data[name] = self.parse_element(current)\n",
    "\n",
    "            elif current.text:\n",
    "                data[name] = current.text\n",
    "\n",
    "        return data\n",
    "\n",
    "    def save_to_xml(self):\n",
    "        now = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        xml_file_path = f\"{now}.xml\"\n",
    "        with open(xml_file_path,\"w\",encoding = \"utf-8\") as x:\n",
    "            x.write(self.xml.decode(\"utf-8\") )\n",
    "            \n",
    "        files = os.listdir(\".\")\n",
    "        newest_file = max(files,key = lambda f:os.path.getmtime(f))\n",
    "        return newest_file\n",
    "            \n",
    "    \n",
    "            \n",
    "\n",
    "\n",
    "# Set this to your API key, either save to an environment variable or put in plain text\n",
    "# be careful if its in plain text, this is your secret key!\n",
    "# API_KEY = os.environ.get('bbb630ca6698171a44ddc9ebe12ad4a5f0d32bb9')\n",
    "API_KEY = 'bbb630ca6698171a44ddc9ebe12ad4a5f0d32bb9'\n",
    "\n",
    "bods = BODSClient(api_key=API_KEY)\n",
    "\n",
    "# Same bounding box as in other examples\n",
    "box = BoundingBox(min_longitude=-1.203175, min_latitude=52.914955, max_longitude=-1.088463, max_latitude=53.018299)\n",
    "siri_params = SIRIVMParams(bounding_box=box)\n",
    "\n",
    "# This will save a zip to the current working folder every 15 seconds forever...\n",
    "# decide for yourself how often you want to collect data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    \n",
    "    #获取指定区域的siri-VM数据\n",
    "    data = bods.get_siri_vm_data_feed(params = siri_params)\n",
    "\n",
    "    siri = Siri(data)\n",
    "    newest_file = siri.save_to_xml()\n",
    "    newest_file_name = os.path.basename(newest_file)\n",
    "    \n",
    "    OUTPUT = f\"{newest_file_name}.csv\"   \n",
    "    if newest_file.endswith(\".xml\"):\n",
    "        video = parse_xml(newest_file)\n",
    "        savetoCSV(video, OUTPUT)\n",
    "    \n",
    "    xml_path_list = glob.glob(os.path.join(current_path,\"*.xml\"))\n",
    "    newest_xml_list = sorted(xml_path_list,key = os.path.getmtime,reverse = True)\n",
    "    \n",
    "    if newest_xml_list:\n",
    "        newest_xml = newest_xml_list[0]\n",
    "        os.remove(newest_xml)\n",
    "        \n",
    "        \n",
    "    print('saved to csv...')\n",
    "    time.sleep(120)\n",
    "\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb0f8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_siri = pd.read_csv(r\"D:\\诺丁汉大学 学习资料\\诺丁汉大学  计算机科学 学习资料\\毕业设计\\毕业设计——BODS CODE + DATA\\Siri_8_1.csv\")\n",
    "df_service_line = pd.read_csv(r\"D:\\诺丁汉大学 学习资料\\诺丁汉大学  计算机科学 学习资料\\毕业设计\\毕业设计——BODS CODE + DATA\\2023-07-27\\service_line_extract.csv\")\n",
    "df_Naptan_raw = pd.read_csv(r\"D:\\诺丁汉大学 学习资料\\诺丁汉大学  计算机科学 学习资料\\毕业设计\\毕业设计——BODS CODE + DATA\\NAPTAN\\Stops(updated).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb5d639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53dcdc27",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444669b5",
   "metadata": {},
   "source": [
    "# #Handle missing value, Outlier and duplicate value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f27485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siri\n",
    "# 处理缺失值\n",
    "df_siri = df_siri.fillna(value = np.nan)\n",
    "\n",
    "# missing_value = df_siri.isnull()\n",
    "# for column in df_siri.columns:\n",
    "#     print(column+\": \")\n",
    "#     print(\"missingvalue: \"+\"\\n\"+missing_value[column].value_counts().to_string())\n",
    "#     print(\"\\n\")\n",
    "\n",
    "# 因为Siri 缺失值无法用中值等代替且缺失值数量少，所以直接删掉有空值的行\n",
    "df_siri = df_siri.dropna(axis = 0)\n",
    "\n",
    "#将siri数据表中RecordedAtTime变量的值格式化为：xxxx/xx/xx\n",
    "df_siri[\"RecordedAtTime\"] = pd.to_datetime(df_siri[\"RecordedAtTime\"])\n",
    "# df_siri[\"RecordedAtTime2\"] = df_siri[\"RecordedAtTime\"].dt.strftime(\"%Y/%m/%d\")\n",
    "df_siri.insert(0,\"Formatted_RecordedAtTime\",df_siri[\"RecordedAtTime\"].dt.strftime(\"%Y/%m/%d\"))\n",
    "df_siri.drop(columns = [\"RecordedAtTime\"],inplace = True)\n",
    "df_siri = df_siri.drop_duplicates()\n",
    "\n",
    "# Naptan 预处理\n",
    "df_Naptan = df_Naptan_raw[[\"CommonName\",\"Longitude\",\"Latitude\"]]\n",
    "df_Naptan = df_Naptan.drop_duplicates()\n",
    "df_Naptan.dropna(subset=None, inplace =True)\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8faec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_service_line = df_service_line.fillna(value = np.nan)\n",
    "# missing_value = df_service_line.isnull()\n",
    "# for column in df_service_line.columns:\n",
    "#     print(str(column)+\" missing value:\")\n",
    "#     print(missing_value[column].value_counts())\n",
    "#     print(\"\\n\")\n",
    "\n",
    "# 删掉df_service_line[\"TradingName\"] 这个变量没用\n",
    "df_service_line = df_service_line.drop(columns = ['TradingName'])\n",
    "df_service_line = df_service_line.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1492a29e",
   "metadata": {},
   "source": [
    "# Match Siri to TXC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86eae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用python 将CSV数据导入到 数据库：\n",
    "# 以下程序 目前 成功将 5个SIRI文件写入到数据库中的同一个数据表中, 等所有SIRI数据收集完了 就可以通过这个程序将所有SIRI数据导入数据库并合并为一个表格！！！\n",
    "from sqlalchemy  import create_engine\n",
    "\n",
    "config = { \"user\":\"root\",\n",
    "           \"password\":\"Asong22880544#\",\n",
    "           \"host\":\"localhost\",\n",
    "           \"database\":\"siri_data\"    \n",
    "         }\n",
    "\n",
    "connection = mysql.connector.connect(**config)\n",
    "if connection:\n",
    "    print(\"connect database successfully\")\n",
    "\n",
    "\n",
    "# 目前只是选取了5个SIRI文件上传测试一下，后面请将下面路径更改 以上传全部的SIRI文件\n",
    "csv_files = [f for f in os.listdir(\"D:\\诺丁汉大学 学习资料\\诺丁汉大学  计算机科学 学习资料\\毕业设计\\毕业设计——BODS CODE + DATA\") if f.endswith(\".csv\")]\n",
    "\n",
    "\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    df.columns = ['RecordedAtTime', 'ItemIdentifier', 'LineRef', 'DirectionRef',\n",
    "       'DatedVehicleJourneyRef', 'PublishedLineName', 'OperatorRef',\n",
    "       'OriginRef', 'DestinationRef', 'OriginAimedDepartureTime', 'Longitude',\n",
    "       'Latitude', 'DestinationAimedArrivalTime', 'VehicleRef', 'JourneyCode',\n",
    "       'VehicleUniqueId']\n",
    "    \n",
    "    conn = create_engine(\"mysql+mysqlconnector://root:Asong22880544#@localhost:3306/siri_data\")\n",
    "    \n",
    "    df.to_sql(\"SIRI\",con = conn,if_exists = \"append\",index = False)\n",
    "\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954feb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用python 将CSV数据导入到 数据库：\n",
    "# 以下程序 为了将stop level 时间表导入到数据库\n",
    "from sqlalchemy  import create_engine,types\n",
    "\n",
    "config = { \"user\":\"root\",\n",
    "           \"password\":\"Asong22880544#\",\n",
    "           \"host\":\"localhost\",\n",
    "           \"database\":\"siri_data\"    \n",
    "         }\n",
    "\n",
    "connection = mysql.connector.connect(**config)\n",
    "if connection:\n",
    "    print(\"connect database successfully\")\n",
    "\n",
    "cur = connection.cursor()\n",
    "\n",
    "\n",
    "# csv_files = [f for f in os.listdir(\"D:\\诺丁汉大学 学习资料\\诺丁汉大学  计算机科学 学习资料\\毕业设计\\毕业设计——BODS CODE + DATA\\Stop Level 数据\\Different operators stop level\\inbound_timetable_folder\") if f.endswith(\".csv\")]\n",
    "\n",
    "\n",
    "# for file in csv_files:\n",
    "#     df = pd.read_csv(file)\n",
    "#     df.columns = ['RecordedAtTime', 'ItemIdentifier', 'LineRef', 'DirectionRef',\n",
    "#        'DatedVehicleJourneyRef', 'PublishedLineName', 'OperatorRef',\n",
    "#        'OriginRef', 'DestinationRef', 'OriginAimedDepartureTime', 'Longitude',\n",
    "#        'Latitude', 'DestinationAimedArrivalTime', 'VehicleRef', 'JourneyCode',\n",
    "#        'VehicleUniqueId']\n",
    "    \n",
    "conn = create_engine(\"mysql+mysqlconnector://root:Asong22880544#@localhost:3306/siri_data\")\n",
    "\n",
    "# dtype_mapping = {\n",
    "#                 \"Sequence Number\":types.TEXT(200), \n",
    "#                 \"Stop Point Ref\":types.TEXT,\n",
    "#                 \"Latitude\":types.TEXT(200), \n",
    "#                 \"Longitude\":types.TEXT(200), \n",
    "#                  \"Common Name\":types.TEXT(200)\n",
    "#                 }\n",
    "\n",
    "\n",
    "\n",
    "root_files1 = \"D:\\诺丁汉大学 学习资料\\诺丁汉大学  计算机科学 学习资料\\毕业设计\\毕业设计——BODS CODE + DATA\\Stop Level 数据\\Different operators stop level\\inbound_timetable_folder\"\n",
    "root_files2 = \"D:\\诺丁汉大学 学习资料\\诺丁汉大学  计算机科学 学习资料\\毕业设计\\毕业设计——BODS CODE + DATA\\Stop Level 数据\\Different operators stop level\\outbound_timetable_folder\"\n",
    "\n",
    "\n",
    "def load_timetables_csv(root_files):\n",
    "    for root,dirs,files, in os.walk(root_files):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                file_path =os.path.join(root_files,file)\n",
    "                df = pd.read_csv(file_path)\n",
    "                dtype_mapping = {}\n",
    "                for column in df.columns:\n",
    "                    dtype_mapping[f\"{column}\"] = types.TEXT(50)\n",
    "                    \n",
    "                table_name = os.path.splitext(file)[0]\n",
    "                \n",
    "                \n",
    "                try:\n",
    "                    df.to_sql(table_name.lower(),con = conn,if_exists = \"replace\",index = False,dtype= dtype_mapping)\n",
    "                    \n",
    "                except  mysql.connector.errors.ProgrammingError as e:\n",
    "                        if \"Row size too large\" in str(e):\n",
    "                            print(f\"Skipped {table_name} due to Row size too large error.\")\n",
    "                        else: print(f\"error while uploading {table_name}:{e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"errors while uploding {table_name} errors:{e}\")\n",
    "    \n",
    "    print(\"successfully import\")\n",
    "    \n",
    "#     engine.dispose()\n",
    "    \n",
    "    \n",
    "      \n",
    "load_timetables_csv(root_files1)\n",
    "load_timetables_csv(root_files2)\n",
    "conn.close()   \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d4e224",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 将stop level所有文件下的CSV文件提取出来，方便处理\n",
    "\n",
    "def extract_csv(source_folder_path,dest_folder_path):\n",
    "    for dir in os.listdir(source_folder_path):\n",
    "        floder_path = os.path.join(source_folder_path,dir)\n",
    "        \n",
    "        if os.path.isdir(floder_path):\n",
    "            files  = os.listdir(floder_path)\n",
    "            for file in files:\n",
    "                if file.endswith(\".csv\"):\n",
    "                    file_path = os.path.join(floder_path,file)\n",
    "                    shutil.copy(file_path,dest_folder_path)\n",
    "\n",
    "\n",
    "source_folder_path = \"D:\\诺丁汉大学 学习资料\\诺丁汉大学  计算机科学 学习资料\\毕业设计\\毕业设计——BODS CODE + DATA\\Stop Level 数据\\Different operators stop level\\outbound_timetable_folder\"\n",
    "dest_folder_path = \"D:\\诺丁汉大学 学习资料\\诺丁汉大学  计算机科学 学习资料\\毕业设计\\毕业设计——BODS CODE + DATA\\Stop Level 数据\\Different operators stop level\\outbound_timetable_folder\"\n",
    "         \n",
    "extract_csv(source_folder_path,dest_folder_path)\n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc301dee",
   "metadata": {},
   "source": [
    "# Matching all of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2125723",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Napatan = pd.read_csv(\"H:\\LI_SONG_BODS\\8.12\\8.12\\Stops(updated).csv\")\n",
    "# df_timetables_1 = pd.read_csv(r\"D:\\诺丁汉大学 学习资料\\诺丁汉大学  计算机科学 学习资料\\毕业设计\\毕业设计——BODS CODE + DATA\\测试数据\\timetables\\3A_inbound.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6aff557",
   "metadata": {},
   "source": [
    "# #The First step: trying to pair all of the  timetable with NAPATAN first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0ab4f6b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108_outbound.csv\n",
      "11_12_outbound(1).csv\n",
      "11_12_outbound(2).csv\n",
      "123_outbound.csv\n",
      "15_outbound.csv\n",
      "197_outbound.csv\n",
      "1_outbound.csv\n",
      "204_outbound.csv\n",
      "205_outbound.csv\n",
      "217_outbound.csv\n",
      "219_outbound.csv\n",
      "2_outbound.csv\n",
      "31_outbound.csv\n",
      "32_outbound.csv\n",
      "33_outbound (2).csv\n",
      "33_outbound.csv\n",
      "34_outbound.csv\n",
      "354_outbound.csv\n",
      "35_outbound.csv\n",
      "3A_outbound.csv\n",
      "3B_outbound.csv\n",
      "3C_outbound.csv\n",
      "417_outbound.csv\n",
      "510_outbound.csv\n",
      "511_outbound.csv\n",
      "528_outbound.csv\n",
      "532_outbound.csv\n",
      "535_outbound.csv\n",
      "536_outbound.csv\n",
      "5_outbound.csv\n",
      "6.0_6.1_6.2_6.3_6.4_6E_6N_6X_outbound(1).csv\n",
      "6.0_6.1_6.2_6.3_6.4_6E_6N_6X_outbound(10).csv\n",
      "6.0_6.1_6.2_6.3_6.4_6E_6N_6X_outbound(11).csv\n",
      "6.0_6.1_6.2_6.3_6.4_6E_6N_6X_outbound(12).csv\n",
      "6.0_6.1_6.2_6.3_6.4_6E_6N_6X_outbound(13).csv\n",
      "6.0_6.1_6.2_6.3_6.4_6E_6N_6X_outbound(14).csv\n",
      "6.0_6.1_6.2_6.3_6.4_6E_6N_6X_outbound(15).csv\n",
      "6.0_6.1_6.2_6.3_6.4_6E_6N_6X_outbound(16).csv\n",
      "6.0_6.1_6.2_6.3_6.4_6E_6N_6X_outbound(2).csv\n",
      "6.0_6.1_6.2_6.3_6.4_6E_6N_6X_outbound(3).csv\n",
      "6.0_6.1_6.2_6.3_6.4_6E_6N_6X_outbound(4).csv\n",
      "6.0_6.1_6.2_6.3_6.4_6E_6N_6X_outbound(5).csv\n",
      "6.0_6.1_6.2_6.3_6.4_6E_6N_6X_outbound(6).csv\n",
      "6.0_6.1_6.2_6.3_6.4_6E_6N_6X_outbound(7).csv\n",
      "6.0_6.1_6.2_6.3_6.4_6E_6N_6X_outbound(8).csv\n",
      "6.0_6.1_6.2_6.3_6.4_6E_6N_6X_outbound(9).csv\n",
      "747_outbound.csv\n",
      "850_outbound.csv\n",
      "852_outbound.csv\n",
      "853_outbound.csv\n",
      "863_outbound.csv\n",
      "9.1_9.3_outbound(1).csv\n",
      "9.1_9.3_outbound(2).csv\n",
      "90_outbound.csv\n",
      "9_outbound.csv\n",
      "CC_outbound.csv\n",
      "CMT_outbound(1).csv\n",
      "CMT_outbound(2).csv\n",
      "COT_outbound.csv\n",
      "CS_outbound.csv\n",
      "C_outbound.csv\n",
      "H1_outbound.csv\n",
      "HQ_outbound.csv\n",
      "i4_outbound.csv\n",
      "IF_outbound.csv\n",
      "IGO_outbound.csv\n",
      "KC_outbound.csv\n",
      "RA_outbound.csv\n",
      "RM_outbound.csv\n",
      "RV1_outbound.csv\n",
      "SKY_outbound.csv\n",
      "SNX_outbound.csv\n",
      "SN_outbound.csv\n",
      "SP_outbound.csv\n",
      "SW_outbound.csv\n",
      "TA_outbound.csv\n",
      "TM_outbound.csv\n",
      "TWO_outbound.csv\n",
      "V1_outbound(1).csv\n",
      "V1_outbound(2).csv\n",
      "V3_outbound.csv\n",
      "X38_outbound.csv\n",
      "outbound_timetables_csv match successfully\n"
     ]
    }
   ],
   "source": [
    "# 将所有的timetables与NAPTAN匹配\n",
    "inbound_timetables_csv = [file for file in os.listdir(\"H:\\LI_SONG_BODS\\8.12\\8.12\\Different operators stop level\\inbound_timetable_folder\")]\n",
    "outbound_timetables_csv = [file for file in os.listdir(\"H:\\LI_SONG_BODS\\8.12\\8.12\\Different operators stop level\\outbound_timetable_folder\")]\n",
    "\n",
    "# for f in inbound_timetables_csv:\n",
    "#     print(f)\n",
    "#     df = pd.read_csv(f\"H:\\LI_SONG_BODS\\8.12\\8.12\\Different operators stop level\\inbound_timetable_folder\\{f}\")\n",
    "#     for row in df.iloc[5:][\"Stop Point Ref\"]:\n",
    "#         for index,ATCOCode in df_Napatan[\"ATCOCode\"].iteritems():\n",
    "#             if row == ATCOCode:\n",
    "#     #             condition = df_Napatan\n",
    "#                 df.loc[df[\"Stop Point Ref\"] == row,\"Latitude\"] = df_Napatan.iloc[index][\"Latitude\"]\n",
    "#                 df.loc[df[\"Stop Point Ref\"] == row,\"Longitude\"] = df_Napatan.iloc[index][\"Longitude\"]\n",
    "#     df.to_csv(f\"H:\\LI_SONG_BODS\\8.12\\8.12\\STOP_LEVEL(MATCHED)\\{f}\",index = False)\n",
    "# print(\"inbound_timetables_csv match successfully\")\n",
    "\n",
    "# for f in outbound_timetables_csv:\n",
    "#     print(f)\n",
    "#     df = pd.read_csv(f\"H:\\LI_SONG_BODS\\8.12\\8.12\\Different operators stop level\\outbound_timetable_folder\\{f}\")\n",
    "#     for row in df.iloc[5:][\"Stop Point Ref\"]:\n",
    "#         for index,ATCOCode in df_Napatan[\"ATCOCode\"].iteritems():\n",
    "#             if row == ATCOCode:\n",
    "#     #             condition = df_Napatan\n",
    "#                 df.loc[df[\"Stop Point Ref\"] == row,\"Latitude\"] = df_Napatan.iloc[index][\"Latitude\"]\n",
    "#                 df.loc[df[\"Stop Point Ref\"] == row,\"Longitude\"] = df_Napatan.iloc[index][\"Longitude\"]\n",
    "#     df.to_csv(f\"H:\\LI_SONG_BODS\\8.12\\8.12\\STOP_LEVEL(MATCHED)\\{f}\",index = False)\n",
    "# print(\"outbound_timetables_csv match successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ece20886",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_siri_final = pd.read_csv(r\"D:\\诺丁汉大学 学习资料\\诺丁汉大学  计算机科学 学习资料\\毕业设计\\毕业设计——BODS CODE + DATA\\Final Data\\SIRI_inbound.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bca63f1",
   "metadata": {},
   "source": [
    "# Then, for all of the SIRI data, pair each row of data with that timetable, and output the expected time of each Siri and corresponding site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3ddcf09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_siri = pd.read_csv(r\"H:\\LI_SONG_BODS\\OneDrive_2023-08-22\\Final Data\\test_inbound.csv\")\n",
    "df_siri[\"Common Name\"] = \"\"\n",
    "df_siri[\"Expected Arrival Time\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "31749ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timetables_target = pd.read_csv(r\"H:\\LI_SONG_BODS\\OneDrive_2023-08-22\\Final Data\\stop_level(matched)\\outbound\\TWO_outbound.csv\")\n",
    "df_timetables_target.replace(\"-\",np.nan,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83a1bcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_approximately_equal(a,b,tolerance=0.0015):\n",
    "    diff = abs(a-b)\n",
    "    return \"yes\" if diff <= tolerance else \"no\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1bed7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 调整思路，重新匹配：选择几条线路来一条一条进行匹配\n",
    "\n",
    "# inbound_DirectionRef = [\"in\",\"inbound\"]\n",
    "# outbound_DirectionRef = [\"out\",\"outbound\"]\n",
    "# df_siri_final_result = pd.DataFrame()\n",
    "\n",
    "\n",
    "# for line_index1,row1 in df_siri.iterrows():\n",
    "#     if row1[\"Expected Arrival Time\"]==\"\":\n",
    "#         #匹配 journey\n",
    "#         for column_index,column_name in enumerate(df_timetables_target.iloc[2]):\n",
    "#             if str(row1[\"DatedVehicleJourneyRef\"]) == str(df_timetables_target.iloc[2,column_index])+str(\".0\"):\n",
    "#                 print(\"Matched journey \"+str(row1[\"DatedVehicleJourneyRef\"]))\n",
    "#                 print(\"\\n\")\n",
    "\n",
    "#                 #确定具体站点\n",
    "#                 #Determine specific station\n",
    "#                 df_matched_journey = pd.DataFrame(df_timetables_target.iloc[:,[0,1,2,3,4,column_index]])\n",
    "#                 for line_index2,row2 in df_matched_journey.iloc[5:].iterrows():\n",
    "#                     if pd.notna(df_matched_journey.iloc[line_index2][\"Longitude\"]) and pd.notna(df_matched_journey.iloc[line_index2][\"Latitude\"]):\n",
    "                        \n",
    "                        \n",
    "#                         if are_approximately_equal(float(row1[\"Longitude\"]),float(df_matched_journey.iloc[line_index2][\"Longitude\"])) == \"yes\" and are_approximately_equal(float(row1[\"Latitude\"]),float(df_matched_journey.iloc[line_index2][\"Latitude\"])) == \"yes\":\n",
    "# #                             print(\"yes\")\n",
    "\n",
    "#                             df_siri.loc[line_index1,\"Common Name\"] = df_matched_journey.iloc[line_index2][\"Common Name\"]\n",
    "#                             df_siri.loc[line_index1,\"Expected Arrival Time\"] = df_matched_journey.iloc[line_index2][5]\n",
    "                            \n",
    "\n",
    "                            \n",
    "                            \n",
    "# print(\"over\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "814aa7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_siri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b20eee78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_siri.to_csv(r\"H:\\LI_SONG_BODS\\OneDrive_2023-08-22\\Final Data\\Matched_siri\\outbound\\TWO_outbound_matched(2).csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ad6539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for line_index1,row1 in df_siri.iterrows():\n",
    "    \n",
    "#     #确定这一行是i4 和 inbound\n",
    "#     if row1[\"LineRef\"] = \"i4\" and row1[\"DirectionRef\"].lower() in inbound_DirectionRef:\n",
    "        \n",
    "#         #确定journey\n",
    "#         #Determine which journey for each SIRI line\n",
    "#         for column_index,column_name in enumerate(df_timetables_target.iloc[2]):\n",
    "\n",
    "#               if str(row1[\"DatedVehicleJourneyRef\"]) == str(df_timetables_target.iloc[2,column_index]):\n",
    "#                     print(\"Matched journey \"+str(row[\"DatedVehicleJourneyRef\"]))\n",
    "#                     print(\"\\n\")\n",
    "\n",
    "#                     #确定具体站点\n",
    "#                     #Determine specific station\n",
    "#                     df_matched_journey = pd.DataFrame(df_timetables_target.iloc[:,[0,1,2,3,4,column_index]])\n",
    "#                     for line_index2,row2 in df_matched_journey.iloc[5:].iterrows():\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "#                                     if df_matched_journey.iloc[line_index2][5] !=\"-\":\n",
    "#                                         print(df_matched_journey.iloc[line_index2][5])\n",
    "#                                         if df_matched_journey.iloc[line_index2][\"Longitude\"]!=\"-\" and df_matched_journey.iloc[line_index2][\"Latitude\"]!=\"-\":\n",
    "                                            \n",
    "#                                             if are_approximately_equal(float(row[\"Longitude\"]),float(df_matched_journey.iloc[line_index2][\"Longitude\"])) and are_approximately_equal(float(row[\"Latitude\"]),float(df_matched_journey.iloc[line_index2][\"Latitude\"])):\n",
    "#                                                 row[\"Common Name\"] = df_matched_journey.iloc[line_index2][\"Common Name\"]\n",
    "#                                                 row[\"Expected Arrival Time\"] = df_matched_journey.iloc[line_index2][5]\n",
    "#                                                 df_siri_final_result = pd.concat([df_siri_final_result,row],axis = 0)\n",
    "#                                                 print(\"matched successfully\")\n",
    "                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbd81ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# df_siri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032e56c2",
   "metadata": {},
   "source": [
    "# Maching learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1db09b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_siri_final = pd.read_csv(r\"H:\\LI_SONG_BODS\\OneDrive_2023-09-03\\Final Data\\mergeted_date(4).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ce69c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理 去重复值\n",
    "df_siri_final.drop(columns = [\"RecordedAtTime\"],inplace = True)\n",
    "df_siri_final = df_siri_final.dropna()\n",
    "\n",
    "\n",
    "#one_hot encoding\n",
    "columns = [\"LineRef\",\"DirectionRef\",\"OperatorRef\",\"time slot\",\"Status\"]\n",
    "\n",
    "for column in columns:\n",
    "    dummy_df = pd.get_dummies(df_siri_final[column],prefix=column)\n",
    "    df_siri_final = pd.concat([df_siri_final,dummy_df],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "688425ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 相关性较低的变量：DatedVehicleJourneyRef, JourneyCode,Common Name,Date,Real time,Expected Arrival Time,Deviation,DirectionRef\n",
    "df_siri_final.drop(columns = [\"DatedVehicleJourneyRef\",\"JourneyCode\",\"LineRef\",\"DirectionRef\",\"OperatorRef\",\"Common Name\",\"Date\",\"time slot\",\"Status\",\"Real time\",\"Expected Arrival Time\",\"Deviation\"],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1aded069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征工程 相关性分析\n",
    "# columns_name = df_siri_final.columns.tolist()\n",
    "# target_columns = [\"Status_['Delay']\",\"Status_['On time']\",\"Status_['arrive early']\"]\n",
    "\n",
    "# for name in columns_name:\n",
    "#     for column in target_columns:\n",
    "#         correlation = df_siri_final[[name,column]].corr()\n",
    "#         print(name+\"  \"+column)\n",
    "#         print(\"\\n\")\n",
    "#         print(str(correlation))\n",
    "#         print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "861e1da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小: (17220, 18)\n",
      "测试集大小: (4305, 18)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 假设你的数据集为 df_siri_final\n",
    "\n",
    "# 定义特征列和目标变量\n",
    "X = df_siri_final.drop(columns=[\"Status_['Delay']\",\"Status_['On time']\",\"Status_['arrive early']\"])  # 特征列（去除目标变量）\n",
    "y = df_siri_final[[\"Status_['Delay']\",\"Status_['On time']\",\"Status_['arrive early']\"]]  # 目标变量\n",
    "\n",
    "# 划分数据集为训练集和测试集\n",
    "# test_size 指定测试集的比例，random_state 设置随机种子以确保可重复性\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 打印训练集和测试集的大小\n",
    "print(\"训练集大小:\", X_train.shape)\n",
    "print(\"测试集大小:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39e74251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将y_train 转换为3个一维数组\n",
    "y_train_delay= y_train.iloc[:,0]\n",
    "y_train_on_time= y_train.iloc[:,1]\n",
    "y_train_arrive_early= y_train.iloc[:,2]\n",
    "\n",
    "y_test_delay= y_test.iloc[:,0]\n",
    "y_test_on_time= y_test.iloc[:,1]\n",
    "y_test_arrive_early= y_test.iloc[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bbcc3b",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbour Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a8e945",
   "metadata": {},
   "source": [
    "delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ec42429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: accuracy = 0.7177700348432056\n",
      "Fold 2: accuracy = 0.7305458768873403\n",
      "Fold 3: accuracy = 0.743321718931475\n",
      "Fold 4: accuracy = 0.7317073170731707\n",
      "Fold 5: accuracy = 0.7398373983739838\n",
      "Mean scores:  0.7326364692218351\n",
      "delay Accuracy: 0.7660859465737514\n",
      "delay Best Scores:  0.7562137049941928\n",
      "程序运行时间为： 29.145756721496582 秒\n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "delay_knn = KNeighborsClassifier()\n",
    "\n",
    "# define parameter grid\n",
    "param_grid = {\n",
    "    'n_neighbors': range(1, 21),  \n",
    "    'weights': ['uniform', 'distance'],  \n",
    "    'p': [1, 2]  \n",
    "}\n",
    "\n",
    "#Randomized Search Optimization Parameters\n",
    "delay_knn = RandomizedSearchCV(delay_knn, param_grid, n_iter=10, cv=5)\n",
    "\n",
    "#Cross-validation to evaluate model performance\n",
    "scores = cross_val_score(delay_knn,X_test,y_test_delay,cv = 5,scoring = 'accuracy')\n",
    "for fold, score in enumerate(scores):\n",
    "    print(f\"Fold {fold +1}: accuracy = {score}\")\n",
    "mean_score = scores.mean()\n",
    "print(\"Mean scores: \",mean_score)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "#Model Training\n",
    "delay_knn.fit(X_train,y_train_delay)\n",
    "delay_knn_pred = delay_knn.predict(X_test)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "delay_knn_acc = accuracy_score(y_test_delay, delay_knn_pred)\n",
    "print('delay Accuracy:', delay_knn_acc)\n",
    "print(\"delay Best Scores: \" , delay_knn.best_score_)\n",
    "print(\"程序运行时间为：\",end_time - start_time, \"秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016b892d",
   "metadata": {},
   "source": [
    "Arrive Delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bcbca33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: accuracy = 0.7224157955865272\n",
      "Fold 2: accuracy = 0.7479674796747967\n",
      "Fold 3: accuracy = 0.7212543554006968\n",
      "Fold 4: accuracy = 0.7073170731707317\n",
      "Fold 5: accuracy = 0.7212543554006968\n",
      "Mean scores:  0.7240418118466899\n",
      "delay Accuracy: 0.7586527293844367\n",
      "delay Best Scores:  0.7447735191637631\n",
      "程序运行时间为： 36.45775604248047 秒\n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "Arrive_Early_knn = KNeighborsClassifier()\n",
    "\n",
    "# define parameter grid\n",
    "param_grid = {\n",
    "    'n_neighbors': range(1, 21),  \n",
    "    'weights': ['uniform', 'distance'],  \n",
    "    'p': [1, 2]  \n",
    "}\n",
    "\n",
    "#Randomized Search Optimization Parameters\n",
    "Arrive_Early_knn = RandomizedSearchCV(Arrive_Early_knn, param_grid, n_iter=10, cv=5)\n",
    "\n",
    "#Cross-validation to evaluate model performance\n",
    "scores = cross_val_score(Arrive_Early_knn,X_test,y_test_arrive_early,cv = 5,scoring = 'accuracy')\n",
    "for fold, score in enumerate(scores):\n",
    "    print(f\"Fold {fold +1}: accuracy = {score}\")\n",
    "mean_score = scores.mean()\n",
    "print(\"Mean scores: \",mean_score)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "#Model Training\n",
    "Arrive_Early_knn.fit(X_train,y_train_arrive_early)\n",
    "Arrive_Early_knn_pred = Arrive_Early_knn.predict(X_test)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "Arrive_Early_knn_acc = accuracy_score(y_test_arrive_early, Arrive_Early_knn_pred)\n",
    "print('delay Accuracy:', Arrive_Early_knn_acc)\n",
    "print(\"delay Best Scores: \" , Arrive_Early_knn.best_score_)\n",
    "print(\"程序运行时间为：\",end_time - start_time, \"秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0053a1",
   "metadata": {},
   "source": [
    "On-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "866bf76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: accuracy = 0.686411149825784\n",
      "Fold 2: accuracy = 0.6840882694541232\n",
      "Fold 3: accuracy = 0.6771196283391405\n",
      "Fold 4: accuracy = 0.6724738675958188\n",
      "Fold 5: accuracy = 0.6759581881533101\n",
      "Mean scores:  0.6792102206736352\n",
      "On_time Accuracy: 0.6908246225319397\n",
      "On_time Best Scores:  0.6813588850174216\n",
      "程序运行时间为： 30.733189821243286 秒\n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "On_time_knn = KNeighborsClassifier()\n",
    "\n",
    "# define parameter grid\n",
    "param_grid = {\n",
    "    'n_neighbors': range(1, 21),  \n",
    "    'weights': ['uniform', 'distance'],  \n",
    "    'p': [1, 2]  \n",
    "}\n",
    "\n",
    "#Randomized Search Optimization Parameters\n",
    "On_time_knn = RandomizedSearchCV(On_time_knn, param_grid, n_iter=10, cv=5)\n",
    "\n",
    "#Cross-validation to evaluate model performance\n",
    "scores = cross_val_score(On_time_knn,X_test,y_test_on_time,cv = 5,scoring = 'accuracy')\n",
    "for fold, score in enumerate(scores):\n",
    "    print(f\"Fold {fold +1}: accuracy = {score}\")\n",
    "mean_score = scores.mean()\n",
    "print(\"Mean scores: \",mean_score)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "#Model Training\n",
    "On_time_knn.fit(X_train,y_train_on_time)\n",
    "On_time_knn_pred = On_time_knn.predict(X_test)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "On_time_knn_acc = accuracy_score(y_test_on_time, On_time_knn_pred)\n",
    "print('On_time Accuracy:', On_time_knn_acc)\n",
    "print(\"On_time Best Scores: \" , On_time_knn.best_score_)\n",
    "print(\"程序运行时间为：\",end_time - start_time, \"秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce4b65a",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b732246d",
   "metadata": {},
   "source": [
    "Delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ec73558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: accuracy = 0.7711962833914053\n",
      "Fold 2: accuracy = 0.7932636469221835\n",
      "Fold 3: accuracy = 0.8025551684088269\n",
      "Fold 4: accuracy = 0.8072009291521487\n",
      "Fold 5: accuracy = 0.7967479674796748\n",
      "Mean scores:  0.7941927990708478\n",
      "Delay Accuracy: 0.8153310104529616\n",
      "Delay Best Scores:  0.7997677119628339\n",
      "程序运行时间为： 4.9541661739349365 秒\n"
     ]
    }
   ],
   "source": [
    "Delay_DecisionTree = DecisionTreeClassifier()\n",
    "\n",
    "# define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "#Grid Search Optimization Parameters\n",
    "Delay_DecisionTree = GridSearchCV(Delay_DecisionTree, param_grid, cv=5)\n",
    "\n",
    "#Cross-validation to evaluate model performance\n",
    "scores = cross_val_score(Delay_DecisionTree,X_test,y_test_delay,cv = 5,scoring = 'accuracy')\n",
    "for fold, score in enumerate(scores):\n",
    "    print(f\"Fold {fold +1}: accuracy = {score}\")\n",
    "mean_score = scores.mean()\n",
    "print(\"Mean scores: \",mean_score)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Model Training\n",
    "Delay_DecisionTree.fit(X_train,y_train_delay)\n",
    "Delay_DecisionTree_pred = Delay_DecisionTree.predict(X_test)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "Delay_DecisionTree_acc = accuracy_score(y_test_delay, Delay_DecisionTree_pred)\n",
    "print('Delay Accuracy:', Delay_DecisionTree_acc)\n",
    "print(\"Delay Best Scores: \" , Delay_DecisionTree.best_score_)\n",
    "\n",
    "print(\"程序运行时间为：\",end_time - start_time, \"秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e232af0a",
   "metadata": {},
   "source": [
    "Arrive Early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "023e73ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: accuracy = 0.7828106852497096\n",
      "Fold 2: accuracy = 0.8141695702671312\n",
      "Fold 3: accuracy = 0.8292682926829268\n",
      "Fold 4: accuracy = 0.775842044134727\n",
      "Fold 5: accuracy = 0.7793263646922184\n",
      "Mean scores:  0.7962833914053425\n",
      "Arrive Early Accuracy: 0.8032520325203252\n",
      "Arrive Early Best Scores:  0.8048199767711962\n",
      "程序运行时间为： 1.284132957458496 秒\n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "Arrive_Early_DecisionTree = DecisionTreeClassifier()\n",
    "\n",
    "# define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "#Randomized Search Optimization Parameters\n",
    "Arrive_Early_DecisionTree = RandomizedSearchCV(Arrive_Early_DecisionTree, param_grid, n_iter=10, cv=5)\n",
    "\n",
    "#Cross-validation to evaluate model performance\n",
    "scores = cross_val_score(Arrive_Early_DecisionTree,X_test,y_test_arrive_early,cv = 5,scoring = 'accuracy')\n",
    "for fold, score in enumerate(scores):\n",
    "    print(f\"Fold {fold +1}: accuracy = {score}\")\n",
    "mean_score = scores.mean()\n",
    "print(\"Mean scores: \",mean_score)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "#Model Training\n",
    "Arrive_Early_DecisionTree.fit(X_train,y_train_arrive_early)\n",
    "Arrive_Early_DecisionTree_pred = Arrive_Early_DecisionTree.predict(X_test)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "Arrive_Early_DecisionTree_acc = accuracy_score(y_test_arrive_early, Arrive_Early_DecisionTree_pred)\n",
    "print('Arrive Early Accuracy:', Arrive_Early_DecisionTree_acc)\n",
    "print(\"Arrive Early Best Scores: \" , Arrive_Early_DecisionTree.best_score_)\n",
    "print(\"程序运行时间为：\",end_time - start_time, \"秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adbfa4c",
   "metadata": {},
   "source": [
    "On-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8034874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: accuracy = 0.6875725900116144\n",
      "Fold 2: accuracy = 0.6991869918699187\n",
      "Fold 3: accuracy = 0.7003484320557491\n",
      "Fold 4: accuracy = 0.7131242740998839\n",
      "Fold 5: accuracy = 0.6991869918699187\n",
      "Mean scores:  0.6998838559814169\n",
      "On_time Accuracy: 0.7261324041811846\n",
      "On_time Best Scores:  0.7262485481997677\n",
      "程序运行时间为： 1.4139513969421387 秒\n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "On_time_DecisionTree = DecisionTreeClassifier()\n",
    "\n",
    "# define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "#Randomized Search Optimization Parameters\n",
    "On_time_DecisionTree = RandomizedSearchCV(On_time_DecisionTree, param_grid, n_iter=10, cv=5)\n",
    "\n",
    "#Cross-validation to evaluate model performance\n",
    "scores = cross_val_score(On_time_DecisionTree,X_test,y_test_on_time,cv = 5,scoring = 'accuracy')\n",
    "for fold, score in enumerate(scores):\n",
    "    print(f\"Fold {fold +1}: accuracy = {score}\")\n",
    "mean_score = scores.mean()\n",
    "print(\"Mean scores: \",mean_score)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "#Model Training\n",
    "On_time_DecisionTree.fit(X_train,y_train_on_time)\n",
    "On_time_DecisionTree_pred = On_time_DecisionTree.predict(X_test)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "On_time_DecisionTree_acc = accuracy_score(y_test_on_time, On_time_DecisionTree_pred)\n",
    "print('On_time Accuracy:', On_time_DecisionTree_acc)\n",
    "print(\"On_time Best Scores: \" , On_time_DecisionTree.best_score_)\n",
    "print(\"程序运行时间为：\",end_time - start_time, \"秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21357c6e",
   "metadata": {},
   "source": [
    "# Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ca549c",
   "metadata": {},
   "source": [
    "Delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40758865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: accuracy = 0.7073170731707317\n",
      "Fold 2: accuracy = 0.735191637630662\n",
      "Fold 3: accuracy = 0.7584204413472706\n",
      "Fold 4: accuracy = 0.759581881533101\n",
      "Fold 5: accuracy = 0.7386759581881533\n",
      "Mean scores:  0.7398373983739838\n",
      "Delay Accuracy: 0.7419279907084785\n",
      "Delay Best Scores:  0.7191056910569106\n",
      "程序运行时间为： 100.52088570594788 秒\n"
     ]
    }
   ],
   "source": [
    "Dealy_log = LogisticRegression(C=0.1)\n",
    "\n",
    "# define parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100], \n",
    "    'penalty': ['l1', 'l2'], \n",
    "    'solver': ['liblinear', 'saga']  \n",
    "}\n",
    "\n",
    "#Grid Search Optimization Parameters\n",
    "Dealy_log = GridSearchCV(Dealy_log, param_grid, cv=5)\n",
    "\n",
    "#Cross-validation to evaluate model performance\n",
    "scores = cross_val_score(Dealy_log,X_test,y_test_delay,cv = 5,scoring = 'accuracy')\n",
    "for fold, score in enumerate(scores):\n",
    "    print(f\"Fold {fold +1}: accuracy = {score}\")\n",
    "mean_score = scores.mean()\n",
    "print(\"Mean scores: \",mean_score)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#Training model\n",
    "Dealy_log.fit(X_train,y_train_delay)\n",
    "Dealy_log_predict = Dealy_log.predict(X_test)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "Dealy_log_acc = accuracy_score(y_test_delay, Dealy_log_predict)\n",
    "print('Delay Accuracy:', Dealy_log_acc)\n",
    "print(\"Delay Best Scores: \" , Dealy_log.best_score_)\n",
    "print(\"程序运行时间为：\",end_time - start_time, \"秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfff1b2",
   "metadata": {},
   "source": [
    "Arrive Early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68617683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: accuracy = 0.7386759581881533\n",
      "Fold 2: accuracy = 0.7514518002322881\n",
      "Fold 3: accuracy = 0.7526132404181185\n",
      "Fold 4: accuracy = 0.7456445993031359\n",
      "Fold 5: accuracy = 0.7514518002322881\n",
      "Mean scores:  0.7479674796747968\n",
      "Arrive_Early Accuracy: 0.7516840882694541\n",
      "Arrive_Early Best Scores:  0.7513937282229964\n",
      "程序运行时间为： 103.37569761276245 秒\n"
     ]
    }
   ],
   "source": [
    "Arrive_Early_log = LogisticRegression(C=0.1)\n",
    "\n",
    "# define parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100], \n",
    "    'penalty': ['l1', 'l2'], \n",
    "    'solver': ['liblinear', 'saga']  \n",
    "}\n",
    "\n",
    "#Grid Search Optimization Parameters\n",
    "Arrive_Early_log = GridSearchCV(Arrive_Early_log, param_grid, cv=5)\n",
    "\n",
    "#Cross-validation to evaluate model performance\n",
    "scores = cross_val_score(Arrive_Early_log,X_test,y_test_arrive_early,cv = 5,scoring = 'accuracy')\n",
    "for fold, score in enumerate(scores):\n",
    "    print(f\"Fold {fold +1}: accuracy = {score}\")\n",
    "mean_score = scores.mean()\n",
    "print(\"Mean scores: \",mean_score)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#Training model\n",
    "Arrive_Early_log.fit(X_train,y_train_arrive_early)\n",
    "Arrive_Early_log_predict = Arrive_Early_log.predict(X_test)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "Arrive_Early_log_acc = accuracy_score(y_test_arrive_early, Arrive_Early_log_predict)\n",
    "print('Arrive_Early Accuracy:', Arrive_Early_log_acc)\n",
    "print(\"Arrive_Early Best Scores: \" , Arrive_Early_log.best_score_)\n",
    "print(\"程序运行时间为：\",end_time - start_time, \"秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaf9b63",
   "metadata": {},
   "source": [
    "On_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d653637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: accuracy = 0.667828106852497\n",
      "Fold 2: accuracy = 0.6852497096399536\n",
      "Fold 3: accuracy = 0.6736353077816493\n",
      "Fold 4: accuracy = 0.6829268292682927\n",
      "Fold 5: accuracy = 0.6747967479674797\n",
      "Mean scores:  0.6768873403019745\n",
      "Arrive_Early Accuracy: 0.6780487804878049\n",
      "Arrive_Early Best Scores:  0.6739837398373985\n",
      "程序运行时间为： 92.4208824634552 秒\n"
     ]
    }
   ],
   "source": [
    "On_time_log = LogisticRegression(C=0.1)\n",
    "\n",
    "# define parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100], \n",
    "    'penalty': ['l1', 'l2'], \n",
    "    'solver': ['liblinear', 'saga']  \n",
    "}\n",
    "\n",
    "#Grid Search Optimization Parameters\n",
    "On_time_log = GridSearchCV(On_time_log, param_grid, cv=5)\n",
    "\n",
    "#Cross-validation to evaluate model performance\n",
    "scores = cross_val_score(On_time_log,X_test,y_test_on_time,cv = 5,scoring = 'accuracy')\n",
    "for fold, score in enumerate(scores):\n",
    "    print(f\"Fold {fold +1}: accuracy = {score}\")\n",
    "mean_score = scores.mean()\n",
    "print(\"Mean scores: \",mean_score)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#Training model\n",
    "On_time_log.fit(X_train,y_train_on_time)\n",
    "On_time_log_predict = On_time_log.predict(X_test)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "On_time_log_acc = accuracy_score(y_test_on_time, On_time_log_predict)\n",
    "print('On_time Accuracy:', On_time_log_acc)\n",
    "print(\"On_time Best Scores: \" , On_time_log.best_score_)\n",
    "print(\"程序运行时间为：\",end_time - start_time, \"秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee737f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4ed5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#下一步：\n",
    "# 1. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
